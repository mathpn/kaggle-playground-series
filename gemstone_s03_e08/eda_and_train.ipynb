{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\"./data/train.csv\")\n",
    "test_data = pd.read_csv(\"./data/test.csv\")\n",
    "\n",
    "y_train = train_data.pop(\"price\")\n",
    "\n",
    "n_train = len(train_data)\n",
    "all_data = pd.concat((train_data, test_data), axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.histplot(y_train, kde=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "y_train_log = np.log1p(y_train)\n",
    "sns.histplot(y_train_log, kde=True)\n",
    "plt.show()\n",
    "ax = stats.probplot(y_train_log, plot=plt)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_columns = all_data.select_dtypes(include=[np.number]).columns.drop(\"id\")\n",
    "n_cols = int(np.ceil(len(numeric_columns) / 3))\n",
    "\n",
    "fig, ax = plt.subplots(nrows=3, ncols=n_cols, figsize=(10, 10), squeeze=False)\n",
    "for i, col_name in enumerate(numeric_columns):\n",
    "    row = i // n_cols\n",
    "    col = i % n_cols\n",
    "    sns.histplot(all_data[col_name], kde=False, ax=ax[row][col])\n",
    "    ax[row][col].set_title(col_name)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_columns = all_data.select_dtypes(include=[np.number]).columns.drop(\"id\")\n",
    "n_cols = int(np.ceil(len(numeric_columns) / 2))\n",
    "\n",
    "fig, ax = plt.subplots(nrows=2, ncols=n_cols, figsize=(8, 8), squeeze=False)\n",
    "for i, col_name in enumerate(numeric_columns):\n",
    "    row = i // n_cols\n",
    "    col = i % n_cols\n",
    "    sns.boxplot(\n",
    "        y=train_data[col_name], x=(y_train_log > y_train_log.median()).astype(int), ax=ax[row][col]\n",
    "    )\n",
    "    ax[row][col].set_title(col_name)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns = all_data.select_dtypes(include=\"object\").columns\n",
    "fig, axes = plt.subplots(ncols=3, figsize=(14, 4), squeeze=True)\n",
    "\n",
    "\n",
    "orders = {\n",
    "    \"cut\": [\"Ideal\", \"Premium\", \"Very Good\", \"Good\", \"Fair\"],\n",
    "    \"color\": [\"D\", \"E\", \"F\", \"G\", \"H\", \"I\", \"J\"],\n",
    "    \"clarity\": [\"FL\", \"IF\", \"VVS1\", \"VVS2\", \"VS1\", \"VS2\", \"SI1\", \"SI2\", \"I1\", \"I2\", \"I3\"],\n",
    "}\n",
    "\n",
    "\n",
    "for col_name, ax in zip(categorical_columns, axes):\n",
    "    sns.boxplot(y=y_train, x=train_data[col_name], ax=ax, order=orders[col_name])\n",
    "    ax.set_title(col_name)\n",
    "    ax.set_axis_on()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's interesting that worse cuts, colors and clarities present a tendency of higher prices. This is completely unintuitive. Likely, this is explained by the interaction with other variables. For instance, good clarity gemstones may be smaller on average, driving the price down."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.jointplot(x=train_data[\"carat\"], y=y_train_log, kind=\"hex\")\n",
    "plt.show()\n",
    "ax = sns.jointplot(x=train_data[\"carat\"], y=y_train_log, hue=train_data[\"clarity\"], hue_order=orders[\"clarity\"], alpha=0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data[\"volume\"] = all_data[\"x\"] * all_data[\"y\"] * all_data[\"z\"]\n",
    "all_data[\"volume\"].round(2)\n",
    "\n",
    "ax = sns.jointplot(x=all_data.iloc[:n_train][\"volume\"], y=y_train_log, hue=all_data.iloc[:n_train][\"clarity\"], hue_order=orders[\"clarity\"], alpha=0.5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the same volume or carat, we can see that there's a tendency that gemstones with worse clarities have lower prices."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's encode ordinal variables and add new variables based on the existing ones. Check XXX for more details about the new variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/competitions/playground-series-s3e8/discussion/389207\n",
    "\n",
    "orders_dict = {\n",
    "    k: {vv: i for i, vv in enumerate(v)}\n",
    "    for k, v in orders.items()\n",
    "}\n",
    "\n",
    "all_data['cut'] = all_data['cut'].apply(lambda x: orders_dict[\"cut\"][x])\n",
    "all_data['color'] = all_data['color'].apply(lambda x:orders_dict[\"color\"][x])\n",
    "all_data['clarity'] = all_data['clarity'].apply(lambda x:orders_dict[\"clarity\"][x])\n",
    "all_data[\"volume\"] = all_data[\"x\"] * all_data[\"y\"] * all_data[\"z\"]\n",
    "all_data[\"surface_area\"] = 2 * (all_data[\"x\"] * all_data[\"y\"] + all_data[\"y\"] * all_data[\"z\"] + all_data[\"z\"] * all_data[\"x\"])\n",
    "# all_data[\"aspect_ratio_xy\"] = all_data[\"x\"] / all_data[\"y\"]\n",
    "# all_data[\"aspect_ratio_yz\"] = all_data[\"y\"] / all_data[\"z\"]\n",
    "# all_data[\"aspect_ratio_zx\"] = all_data[\"z\"] / all_data[\"x\"]\n",
    "all_data[\"diagonal_distance\"] = np.sqrt(all_data[\"x\"] ** 2 + all_data[\"y\"] ** 2 + all_data[\"z\"] ** 2)\n",
    "all_data[\"relative_height\"] = (all_data[\"z\"] - all_data[\"z\"].min()) / (all_data[\"z\"].max() - all_data[\"z\"].min())\n",
    "all_data[\"relative_position\"] = (all_data[\"x\"] + all_data[\"y\"] + all_data[\"z\"]) / (all_data[\"x\"] + all_data[\"y\"] + all_data[\"z\"]).sum()\n",
    "all_data[\"volume_ratio\"] = all_data[\"x\"] * all_data[\"y\"] * all_data[\"z\"] / (all_data[\"x\"].mean() * all_data[\"y\"].mean() * all_data[\"z\"].mean())\n",
    "all_data[\"length_ratio\"] = all_data[\"x\"] / all_data[\"x\"].mean()\n",
    "all_data[\"width_ratio\"] = all_data[\"y\"] / all_data[\"y\"].mean()\n",
    "all_data[\"height_ratio\"] = all_data[\"z\"] / all_data[\"z\"].mean()\n",
    "all_data[\"sphericity\"] = 1.4641 * (6 * all_data[\"volume\"])**(2/3) / (1e-4 + all_data[\"surface_area\"])\n",
    "# all_data[\"compactness\"] = all_data[\"volume\"]**(1/3) / all_data[\"x\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def train_cv_regressor(model_class, X, y, model_params, folds: int = 5, seed: int = 324):\n",
    "    kf = KFold(n_splits=folds, random_state=seed, shuffle=True)\n",
    "    models = []\n",
    "    for i, (train_idx, val_idx) in enumerate(kf.split(X, y)):\n",
    "        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "\n",
    "        model = model_class(**model_params, random_state=seed)\n",
    "        model.fit(X_train, y_train, eval_set=[(X_val, y_val)], verbose=False)\n",
    "        val_pred = model.predict(X_val)\n",
    "        rmse = np.sqrt(mean_squared_error(y_val, val_pred))\n",
    "        print(f\"Fold {i}: RMSE = {rmse:.3f}\")\n",
    "        models.append(model)\n",
    "    return models\n",
    "\n",
    "\n",
    "def train_regressor(model_class, X_train, X_val, y_train, y_val, model_params, seed: int = 324):\n",
    "    model = model_class(**model_params, random_state=seed)\n",
    "    model.fit(X_train, y_train, eval_set=[(X_val, y_val)], verbose=False)\n",
    "    val_pred = model.predict(X_val)\n",
    "    rmse = np.sqrt(mean_squared_error(y_val, val_pred))\n",
    "    print(f\"RMSE = {rmse:.3f}\")\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "import lightgbm as lgbm\n",
    "import catboost as cb\n",
    "\n",
    "xgb_params = {\n",
    "    \"n_estimators\": 10_000,\n",
    "    \"learning_rate\": 0.05,\n",
    "    \"max_depth\": 10,\n",
    "    \"subsample\": 0.8,\n",
    "    \"colsample_bytree\": 0.8,\n",
    "    \"early_stopping_rounds\": 100,\n",
    "    \"objective\": \"reg:squarederror\",\n",
    "}\n",
    "\n",
    "lgb_params = {\n",
    "    'n_estimators': 2_000,\n",
    "    'learning_rate': 0.05,\n",
    "    'max_depth': 10,\n",
    "    'subsample_for_bin': 20000,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'objective': 'regression',\n",
    "}\n",
    "\n",
    "cb_params = {\n",
    "    'n_estimators': 2_000,\n",
    "    'learning_rate': 0.05,\n",
    "    'max_depth': 8,\n",
    "    'loss_function': 'RMSE',\n",
    "}\n",
    "\n",
    "X = all_data.iloc[:n_train]\n",
    "train_cv_regressor(xgb.XGBRegressor, X, y_train, xgb_params)\n",
    "train_cv_regressor(lgbm.LGBMRegressor, X, y_train, lgb_params)\n",
    "train_cv_regressor(cb.CatBoostRegressor, X, y_train, cb_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, RegressorMixin, TransformerMixin, clone\n",
    "\n",
    "\n",
    "class StackingAveragedModels(BaseEstimator, RegressorMixin, TransformerMixin):\n",
    "    def __init__(self, base_models, meta_model, n_folds: int = 5, random_state: int = 342):\n",
    "        self.base_models = base_models\n",
    "        self.meta_model = meta_model\n",
    "        self.n_folds = n_folds\n",
    "        self.random_state = random_state\n",
    "\n",
    "    # We again fit the data on clones of the original models\n",
    "    def fit(self, X, y):\n",
    "        X = X.values\n",
    "        self.base_models_ = [list() for x in self.base_models]\n",
    "        self.meta_model_ = clone(self.meta_model)\n",
    "        kfold = KFold(n_splits=self.n_folds, shuffle=True, random_state=self.random_state)\n",
    "\n",
    "        # Train cloned base models then create out-of-fold predictions\n",
    "        # that are needed to train the cloned meta-model\n",
    "        out_of_fold_predictions = np.zeros((X.shape[0], len(self.base_models)))\n",
    "        for i, model in enumerate(self.base_models):\n",
    "            for train_index, holdout_index in kfold.split(X, y):\n",
    "                instance = clone(model)\n",
    "                self.base_models_[i].append(instance)\n",
    "                instance.fit(\n",
    "                    X[train_index],\n",
    "                    y[train_index],\n",
    "                    eval_set=[(X[holdout_index], y[holdout_index])],\n",
    "                    verbose=False,\n",
    "                )\n",
    "                y_pred = instance.predict(X[holdout_index])\n",
    "                out_of_fold_predictions[holdout_index, i] = y_pred\n",
    "\n",
    "        # Now train the cloned  meta-model using the out-of-fold predictions as new feature\n",
    "        self.meta_model_.fit(out_of_fold_predictions, y)\n",
    "        return self\n",
    "\n",
    "    # Do the predictions of all base models on the test data and use the averaged predictions as\n",
    "    # meta-features for the final prediction which is done by the meta-model\n",
    "    def predict(self, X):\n",
    "        meta_features = np.column_stack(\n",
    "            [\n",
    "                np.column_stack([model.predict(X) for model in base_models]).mean(axis=1)\n",
    "                for base_models in self.base_models_\n",
    "            ]\n",
    "        )\n",
    "        return self.meta_model_.predict(meta_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO rename y_train before\n",
    "y = y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.8, random_state=342)\n",
    "\n",
    "xgb_model = train_regressor(xgb.XGBRegressor, X_train, X_val, y_train, y_val, xgb_params)\n",
    "cb_model = train_regressor(cb.CatBoostRegressor, X_train, X_val, y_train, y_val, cb_params)\n",
    "\n",
    "meta_model = LinearRegression()\n",
    "stacking_models = StackingAveragedModels([xgb_model, cb_model], meta_model=meta_model)\n",
    "stacking_models.fit(X_train, y_train.values)\n",
    "val_pred = stacking_models.predict(X_val)\n",
    "rmse = np.sqrt(mean_squared_error(y_val, val_pred))\n",
    "print(f\"Ensemble RMSE = {rmse:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9db47ae5badfead9fd39188fd18ff2ce368bc487d9efee61e4db9417ef6acdbc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
